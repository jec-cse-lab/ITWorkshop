{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linear_regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lRKLmJ5VNlf",
        "colab_type": "text"
      },
      "source": [
        "#Linear Regression\n",
        "\n",
        "**Linear regression** is a linear model, i.e. a model that assumes a linear relationship between the input variables ($X$) called ```independent``` variables and the single output variable ($y$) called ```dependent``` variable. More specifically, that the output variable, $y$, can be calculated from a linear combination of the input variables ($X$).\n",
        "\n",
        "A linear relationship is a statistical term used to describe a straight-line relationship between a variable and a constant.\n",
        "\n",
        "Linear regression is an attractive model because the representation is so simple.\n",
        "\n",
        "The linear equation assigns one scale factor coefficient to each input value, called ```slope``` and represented by the Greek letter ```Beta``` ($\\beta$). One additional coefficient is also added, giving the line an additional degree of freedom (e.g. moving up and down on a two-dimensional plot) and is often called the ```intercept``` (or the ```bias```) coefficient.\n",
        "\n",
        "---\n",
        "#Simple Linear Regression\n",
        "\n",
        "In a **simple regression problem**, the output variable, $y$, depends on a single input variable $x$.\n",
        "\n",
        "Let us suppose that we are given the following data points -\n",
        "\n",
        "| x   | y  |\n",
        "|:---:|:--:|\n",
        "| 1   | 6  |\n",
        "| 2   | 5  |\n",
        "| 3   | 7  |\n",
        "| 4.5 | 10 |\n",
        "\n",
        "We can draw a ```scatterplot``` with the data and try to fit a line on it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uJfc6_I8UBw",
        "colab_type": "code",
        "outputId": "85df0812-fde6-4790-e715-183e8e17116f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "X = [1,2,3,4.5]\n",
        "y = [6,5,7,10]\n",
        " \n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "sns.set(color_codes=True)\n",
        "ax = sns.regplot(x=X, y=y, truncate=False, scatter_kws={\"color\": \"red\"}, line_kws={\"color\": \"blue\"})\n",
        "ax.set(xlim=(0.9,4.6),ylim=(4.5,10.5))\n",
        "ax.set(xlabel='X', ylabel='y')\n",
        "plt.setp(ax.collections[1], alpha=0.0)\n",
        "plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AwrZoZGGfwX",
        "colab_type": "text"
      },
      "source": [
        "In the graph shown above, the data points are given in red. The blue line is the representation of a linear regression model. The simple linear regression model, which is essentially a line on a $2$-D plane, would be of the form -\n",
        "\n",
        "$$\\tilde{y} = \\beta_{1}x + \\beta_0$$.\n",
        "\n",
        "---\n",
        "Given the above regression model, if someone were to ask me to predict the output $\\tilde{y}$ when input is $x=3.5$, I could use the equation of the regression line to predict $\\tilde{y}=8.11$\n",
        "\n",
        "Given input parameters $x,\\;\\beta_{0}$ and $\\beta_{1}$, implement the following function to predict $\\tilde{y}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQKvXhAMDkFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(x,beta_1,beta_0):\n",
        "  y_hat = None\n",
        "  return y_hat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p04fRodbPWpV",
        "colab_type": "text"
      },
      "source": [
        "We can see that the (```red```) data points don't fall perfectly on the (```blue```) line. For example, at $x=1$, the predicted output, $\\tilde{y}$, of the regression model is $5$, while the true output, $y$, should've been $6$. This difference between the actual output, ${y}$, and the predicted output, $\\tilde{y}$ is an error in our prediction called ```residual``` ($\\epsilon$).\n",
        "\n",
        "*Note that we are using $\\tilde{y}$ to denote the predicted output and $y$ to denote the real output. The difference between $\\tilde{y}$ and $y$ is the residual, denoted by $\\epsilon$.*\n",
        "\n",
        "The actual output, ${y}$, is simply the sum of predicted output, $\\tilde{y}$,  and the residual, $\\epsilon$, i.e. $y = \\tilde{y}+\\epsilon$\n",
        "\n",
        "####**An infinite number of lines can be drawn on a plane of data points by changing the value of $\\beta_1$ and $\\beta_0$.**\n",
        "\n",
        "**How can we find one specific values of each $\\beta_1$ and $\\beta_0$ to choose a single line (for our linear model) out of all these infinite number of lines?**\n",
        "\n",
        "---\n",
        "\n",
        "Suppose we have a dataset of $N$ data points $\\{(x_i, y_i), 1\\:{\\leq}\\:i\\:{\\leq}\\:n\\}$.\n",
        "\n",
        "We can describe the underlying relationship between $x_i$, $\\tilde{y}_i$, $y_i$ and ${\\epsilon}_i$ by -\n",
        "<br />\n",
        "<br />\n",
        "$$y_i={\\tilde{y_{i}}+\\epsilon _{i}}$$\n",
        "\n",
        "$$\\Rightarrow {y_{i}=\\beta_1 x_{i}+\\beta_0+\\epsilon _{i}}$$\n",
        "\n",
        "$$\\Rightarrow {\\epsilon _{i} = y_{i}-\\beta_1 x_{i}-\\beta_0}$$\n",
        "<br />\n",
        "The ```sum of squared residuals```, $Q(\\beta_1,\\beta_0)$, for some fixed value of $\\beta_1$ and $\\beta_0$, gives us a sense of the total residual error in the entire dataset. The function $Q(\\beta_1,\\beta_0)$ is given by -\n",
        "<br />\n",
        "<br />\n",
        "$$Q(\\beta_1,\\beta_0)={\\sum_{i=1}^{N}{\\epsilon _{i}{^2}}}={\\sum_{i=1}^{N}{(y_{i}-\\beta_1 x_{i}-\\beta_0)}^2}$$\n",
        "<br />\n",
        "Our objective in drawing the regression line is to minimize the residual error. \n",
        "\n",
        "Hence, we have to choose $\\beta_1$ and $\\beta_0$ in such a way that $Q(\\beta_1,\\beta_0)$ is minimized. It turns out that setting the value of $\\beta_1$ and $\\beta_0$ using the following equations minimizes $Q(\\beta_1,\\beta_0)$ -\n",
        "<br /><br />\n",
        "$$\\beta_1 = \\frac{\\sum_{i=1}^{N}(x_i - \\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^{N}(x_i - \\bar{x})^2}$$\n",
        "<br />\n",
        "$$\\beta_0 = \\bar{y}-\\beta_1 \\bar{x}$$\n",
        "<br />\n",
        "where ${\\bar {x}}$ and ${\\displaystyle {\\bar {y}}}$ as the average of the $x_i$ and $y_i$, respectively.\n",
        "\n",
        "> Derivations of the above equations is simple and interesting - we already studied how to find maxima and minima of a function with respect to one variable in calculus. Here, we have to minimize with respect to two variables ($\\beta_0$ and $\\beta_1$), and hence, need partial derivatives. The two partial derivatives will give us two equations, which we need to solve to arrive at the above equations. For this experiment, we needn't go into these details.\n",
        "\n",
        "Implement the function below to output ```beta_1``` and ```beta_0``` using the above formula, given parameters ```X``` and ```y```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cdCiSWBD91G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(X,y):\n",
        "  '''\n",
        "  Input -\n",
        "    X - Array of independent input variables\n",
        "    y - Array of dependent output variables\n",
        "  '''\n",
        "  beta_1 = None\n",
        "  beta_0 = None\n",
        "\n",
        "  return beta_1, beta_0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77mwfzR1E6xJ",
        "colab_type": "text"
      },
      "source": [
        "Now that we have created both ```train``` and ```predict``` functions, we can use them both to do some predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOYCpUZOFYHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "beta_1, beta_0 = train(X,y)\n",
        "print(\"When x=3.5, predicted value of y is \"+str(round(predict(3.5,beta_1,beta_0),2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffJKuTkSF3Gr",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "####**Apply Simple Linear Regression to predict the salary of a person given his/her experience in years.**\n",
        "\n",
        "Before we apply linear regression, we should visualize the data to see whether there is a linear relationship between the variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6rfkq5vHNDp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "experience = [1.1,1.3,1.5,2,2.2,2.9,3,3.2,3.2,3.7,3.9,4,4,4.1,4.5,4.9,5.1,5.3,5.9,6,6.8,7.1,7.9,8.2,8.7,9,9.5,9.6,10.3,10.5]\n",
        "salary = [39343,46205,37731,43525,39891,56642,60150,54445,64445,57189,63218,55794,56957,57081,61111,67938,66029,83088,81363,93940,91738,98273,101302,113812,109431,105582,116969,112635,122391,121872]\n",
        "\n",
        "ax = sns.scatterplot(x=experience, y=salary)\n",
        "ax.set(xlabel='Experience', ylabel='Salary')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQKDxm1OIxjH",
        "colab_type": "text"
      },
      "source": [
        "We can visually see from the above ```scatterplot``` that there is a linear relationship between $X$ and $y$.\n",
        "\n",
        "Train a ```simple linear regression``` on the ```experience``` and ```salary``` data using the ```train``` function created above.\n",
        "\n",
        "Once you have trained the model, use the ```predict``` function to predict the expected salary of a person with $10$ years of experience.\n",
        "\n",
        "Pass the parameters to the $2$ functions as necessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jntygePQJ4hX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "beta_1, beta_0 = train(..., ...)\n",
        "y_pred = predict(10, ..., ...)\n",
        "print(\"Expected salary of a person with 10 years experience: $\"+str(round(y_pred,2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTN3hvSVS9NA",
        "colab_type": "text"
      },
      "source": [
        "#Multiple Linear Regression\n",
        " \n",
        "Till now we have seen linear relations between one independent input variable ($X$) and one dependent output variable ($y$).\n",
        " \n",
        "Contrast this with the case where the output variable ($y$) is dependent on multiple independent input variables ($X = \\{x^{(1)}, ..., x^{(m)}\\}$). The linear regression model when we have $1$ output variable depending on multiple input variables is called **Multiple Linear Regression**.\n",
        " \n",
        "The ```multiple linear regression``` model equation is of the form -\n",
        " \n",
        "$$\\tilde y = \\sum_{i=1}^{m}\\beta_i x_i + \\beta_0$$\n",
        " \n",
        "Let us consider the following example, where we will attempt to predict the ```apparent temperature``` ($\\tilde y$), given ```temperature``` ($x^{(1)}$) and ```humidity``` ($x^{(2)}$).\n",
        "\n",
        "First let us initialize our data into a ```Pandas DataFrame``` and print the first $5$ rows of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEQVsV4yTCfE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = {\n",
        "  \"humidity\" : [0.89,0.89,0.83,0.83,0.85,0.95,0.89,0.82,0.72,0.67,0.54,0.55,0.51,0.47,0.46,0.6,0.63,0.69,0.7,0.77,0.76,0.79,0.77,0.62,0.66,0.8,0.79,0.82,0.83,0.85,0.83,0.78,0.72,0.61,0.52,0.46,0.4,0.4,0.37,0.4,0.36,0.43,0.5,0.53,0.55,0.58,0.59,0.6,0.63,0.87,0.83,0.89,0.95,0.93,0.93,0.82,0.79,0.84,0.73,0.67,0.67,0.71,0.74,0.81,0.9,0.86,0.99,0.93,0.96,0.96,0.96,0.96,0.93,0.82,0.93,0.93,0.93,0.93,0.93,0.93,0.8,0.86,0.84,0.77,0.71,0.71,0.71,0.76,0.66,0.7,0.7,0.66,0.71,0.72,0.72,0.75,0.75,0.88,0.76,0.7,0.73,0.76,0.76,0.76,0.71,0.73,0.72,0.7,0.7,0.65,0.62,0.65,0.65,0.7,0.75,0.75,0.82,0.83,0.84,0.85,0.88,0.88,0.91,0.94,0.93,0.92,0.92],\n",
        "  \"temperature\" : [9.47,9.38,8.29,8.76,9.22,7.73,8.77,10.82,13.77,16.02,17.14,17.8,17.33,18.88,18.91,15.39,15.55,14.26,13.14,11.55,11.18,10.12,10.2,10.42,9.91,11.18,7.16,6.11,6.79,7.26,7.8,9.87,12.22,15.09,17.36,19.01,20.04,21.05,21.18,20.12,20.22,20,17.8,16.06,15.02,14.42,14.26,13.77,13.28,8.63,11.25,11.18,10.69,11.11,11.11,12.17,12.76,13.84,16.18,17.52,17.38,17.36,17.21,15.63,13.58,10.91,8.8,8.96,8.2,7.69,7.77,8.2,8.18,7.31,7.64,6.62,6.68,6.09,6.07,6.14,7.13,7.21,7.57,8.9,9.96,9.89,11.07,10.12,11.04,10.65,10.05,9.9,8.79,7.83,7.86,7.32,7.24,5.44,7.2,6.69,6.21,6.11,6.11,6.17,7.22,7.29,7.41,7.96,8.03,9.08,9.05,9.05,9.18,9.07,7.98,8.07,7.89,7.31,7.34,6.62,6.07,8.34,5.18,3.72,4.86,5.21,6.26],\n",
        "  \"apparent_temp\" : [7.39,9.38,5.94,6.98,7.11,5.52,6.53,10.82,13.77,16.02,17.14,17.8,17.33,18.88,18.91,15.39,15.55,14.26,13.14,11.55,11.18,10.12,10.2,10.42,7.57,11.18,5.04,4.82,4.27,5.16,5.53,7.93,12.22,15.09,17.36,19.01,20.04,21.05,21.18,20.12,20.22,20,17.8,16.06,15.02,14.42,14.26,13.77,13.28,5.47,11.25,11.18,10.69,11.11,11.11,12.17,12.76,13.84,16.18,17.52,17.38,17.36,17.21,15.63,13.58,10.91,5.29,5.78,4.61,3.72,4.65,5.07,4.37,6.18,5.04,2.62,3.22,1.65,1.63,1.49,2.69,3.23,3.54,5.16,6.64,6.55,11.07,10.12,11.04,10.65,10.05,7.72,6.82,5.41,6.12,6.21,6.01,5.44,4.55,3.16,2.69,2.8,2.44,2.92,3.83,4.24,4.82,6.07,7.58,9.08,9.05,9.05,9.18,7.46,6.81,6.43,6.13,4.81,5.18,4.11,3.41,8.34,4.11,2.57,3.71,5.21,4.64]\n",
        "}\n",
        " \n",
        "import pandas as pd\n",
        "df = pd.DataFrame(data)\n",
        " \n",
        "print(df.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMzO2go0S-ty",
        "colab_type": "text"
      },
      "source": [
        "Before we try to fit a linear regression model on our data, let us verify visually that a linear relation actually exists between the two independent input variables (```temperature``` and ```humidity```) and the dependent output variable (```apparent temperature```)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dvsI6ehS_V7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " from mpl_toolkits.mplot3d import Axes3D\n",
        "fig = plt.figure(figsize=(9, 6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(df[\"humidity\"], df[\"temperature\"], df[\"apparent_temp\"], c='skyblue', s=20)\n",
        "ax.set_xlabel('Humidity')\n",
        "ax.set_ylabel('Temperature')\n",
        "ax.set_zlabel('Apparent Temperature')\n",
        "ax.view_init(30, 190)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO_QNGVVFRGO",
        "colab_type": "text"
      },
      "source": [
        "The skyblue points in the $3$-D scatterplot above corresponds to all the different data points on our dataset. We can visually see that the data points all *almost* lie on a plane.\n",
        " \n",
        "*If there is a truly linear relation between our $2$ independent and $1$ dependent variables, then all the data points should've lied perfectly on a plane. But the real-world is noisy. There might be errors while sensing the $X$s and $y$ through some sensors. There might be other minor factors that affects these variables.*\n",
        " \n",
        "You might've guessed that the way we plotted a ```best-fitting line``` when we had one independent variable, we will plot (and use for prediction) a ```best-fitting plane``` now that we have $2$ independent variables.\n",
        " \n",
        "The equation corresponding to the plane representing our linear regression model will be of the form -\n",
        " \n",
        "$$\\tilde y = \\beta_2 x^{(2)} + \\beta_1 x^{(1)} + \\beta_0$$\n",
        "\n",
        "More specifically, to predict the $i$-th data point, we will use the equation -\n",
        "\n",
        "$$\\tilde y_i = \\beta_2 x_i^{(2)} + \\beta_1 x_i^{(1)} + \\beta_0$$\n",
        "<br /><br />\n",
        "In general, we will draw a $n$-D ```hyper-plane``` when we have $n$ independent input variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxT9tJd-YIkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        " \n",
        "#Split data into X and y\n",
        "X = df[[\"temperature\",\"humidity\"]]\n",
        "y = df[[\"apparent_temp\"]]\n",
        " \n",
        "model = LinearRegression()\n",
        "model.fit(X,y)\n",
        "print(\"Coefficient beta_2 for temp:\\t\"+str(round(model.coef_[0][0],6)))\n",
        "print(\"Coefficient beta_1 for hum:\\t\"+str(round(model.coef_[0][1],6)))\n",
        "print(\"Coefficient beta_0 for bias:\\t\"+str(round(model.intercept_[0],6)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Qio4eCdr8h8",
        "colab_type": "text"
      },
      "source": [
        "Now that we have trained our linear regression model by figuring out the values of the coefficients $\\beta_0,\\;\\beta_1$, and $\\beta_2$, we can use the trained model to predict the ```apparent temperature``` for unknown ```temperature``` and ```humidity```.\n",
        " \n",
        "Let us predict the ```apparent temperature``` ($\\tilde y$) for ```temperature``` $(x^{(1)}) = 9.36°C$ and ```humidity``` $(x^{(2)}) = 0.86\\;(86\\%)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd4G4lGFz6TF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_data = {\n",
        "  \"temperature\" : [9.36],\n",
        "  \"humidity\": [0.86]\n",
        "}\n",
        "y_hat = model.predict(pd.DataFrame(new_data))\n",
        "print(\"The predicted value is: \"+str(round(y_hat[0][0],2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RQIBFpY1as1",
        "colab_type": "text"
      },
      "source": [
        "Our model has predicted $\\tilde y = 7.75°C$, while the actual value is $y=7.23°C$. There is an error of $0.52°C$, which seems pretty low for day-to-day scenarios.\n",
        " \n",
        "But depending on the application (for example, nuclear power plant), this might be unacceptable. We might need a more complex (nonlinear) model for better accuracy and precision.\n",
        "\n",
        "Along with other algorithms, ```Artificial Neural Networks (ANN)``` can be used to model complex (nonlinear) relations of any degree.\n",
        " \n",
        "#Conclusion\n",
        " \n",
        "In this experiment, you have -\n",
        "*   Implemented and used an algorithm for ```Machine Learning``` - instead of writing an application to predict some value, we taught the computer to predict the value on its own by learning patterns from examples\n",
        "*   Learnt about ```linear regression```, and the difference between ```Simple Linear Regression``` and ```Multiple Linear Regression```\n",
        "*   Learnt the meaning of ```residual``` errors, and observed that the objective in linear regression is to find the coefficients of a linear equation that minimizes the ```sum of squared residuals```\n",
        "*   Learnt to calculate the coefficients $\\beta_1$ and $\\beta_0$ for a ```Simple Linear Regression``` model and use the model to predict a dependent variable ($\\tilde y$).\n",
        "*   Saw how to use ```Pandas``` and ```Scikit-learn``` to train a ```Multiple Linear Regression``` model and use the model to predict the dependent variable ($\\tilde y$) for an unknown data.\n",
        " \n",
        "Kudos! You deserve a round of applause. You have implemented and used an extremely popular algorithm in **predictive data analysis**. There are scores of other algorithms (beyond the scope of this course) that can model your data in much more complex manner to do better predictions.\n",
        " \n",
        "**Data Analysis** and **Machine Learning** are very interesting domains in Computer Science. I urge you to explore the domains on your own, and hope that you will find them interesting enough to join me in learning more about them."
      ]
    }
  ]
}